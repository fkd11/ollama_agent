import feedparser
import requests
import time
import random
import os
from datetime import datetime

query = "heat storage material"
output_dir = "pdf"
os.makedirs(output_dir, exist_ok=True)

BATCH_SIZE = 100
BASE_URL = "http://export.arxiv.org/api/query"

start = 0
total_results = None

while True:
    url = f"{BASE_URL}?search_query=all:{query}&start={start}&max_results={BATCH_SIZE}"
    print(f"\nğŸ” Fetching: start={start}")
    feed = feedparser.parse(url)

    # æœ€åˆã®1å›ã ã‘ãƒˆãƒ¼ã‚¿ãƒ«ä»¶æ•°å–å¾—
    if total_results is None:
        total_results = int(feed.feed.opensearch_totalresults)
        print(f"ğŸ“š Total results: {total_results}")

    if not feed.entries:
        print("âœ… No more entries found. Done.")
        break

    for i, entry in enumerate(feed.entries):
        try:
            title = entry.title.strip()
            authors = [author['name'] for author in entry.authors]
            date = entry.published
            dt = datetime.strptime(date, '%Y-%m-%dT%H:%M:%SZ')
            year = dt.year
            link = entry.links[0].href
            pdf_url = entry.links[1].href

            meta_json = {
                "title": title,
                "authors": authors,
                "link": link,
                "update_date": date,
                "file_name": file_name,
            }
            now = datetime.now()
            print(now)
            print(f"ğŸ“¥ Downloading: {meta_json['title']}")
            

            # ãƒ•ã‚¡ã‚¤ãƒ«åæ•´å½¢
            file_name = title.replace(" ", "_").replace("\n", "").replace("/", "_")
            file_path = os.path.join(output_dir, f"{file_name}.pdf")

            

            # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = requests.get(pdf_url)
            with open(file_path, "wb") as f:
                f.write(response.content)
            
            # ã‚¹ãƒªãƒ¼ãƒ—ã‚’å…¥ã‚Œã‚‹
            time.sleep(random.uniform(1.5, 3.5))

        except Exception as e:
            print(f"âŒ Error downloading '{entry.title}': {e}")

    start += BATCH_SIZE
    if start >= total_results:
        print("âœ… All documents processed.")
        break

    # ãƒãƒƒãƒã”ã¨ã«å°‘ã—é•·ã‚ã«ã‚¹ãƒªãƒ¼ãƒ—
    time.sleep(5)

